---
layout: post
category: "other"
title: "Useful Links"
tags: [useful links,]
---
# **1. Basic Knowledges**
 
## Machine Learning
 
### **树模型**
 
1. **Python & R的树模型**：[A Complete Tutorial on Tree Based Modeling from Scratch (in R & Python)](https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python) 

## Deep Learning 
 
### CNN
 
1. **人脸合成**：[《使用CNN进行人脸合成》](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2651987619&idx=2&sn=dfafcfe8956f4ca686532271cc1b0326&chksm=f1216a52c656e3446fe34187945a48e61ca49bdca187f5c56aab3d59829540e60645949856a4&mpshare=1&scene=1&srcid=1002uGDVOwvDY4eeWR5mzos9&pass_ticket=lD2bnuoAxxbEWgy8KxGVnWVzLL%2FeiSX9MsE68ZdaZQzVoXKXHlCJQ3sVCfTnR7MQ#rd)，代码地址：[https://github.com/zo7/facegen](https://github.com/zo7/facegen)
2. **Residual Net**：[《ICCV 2015 揭开152层神经网络的面纱》](https://mp.weixin.qq.com/s?__biz=MzAwMjM3MTc5OA==&mid=402120336&idx=1&sn=4bdc7abbfe47bc342c86129e1a18ff34&scene=18&mpshare=1&scene=1&srcid=1002wbRSWdlbb0r77vY5iWAo&pass_ticket=DoiMlYDlmCK%2FTS99n6JzBzzsHdN7QoyC81j%2BvUNHFkqqmuADrJsZlH0yXSTgpVEB#rd)
 
### RNN
 
1. **LSTM及其11种变种**：[《图解LSTM神经网络架构及其11种变体（附论文）》](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650719562&idx=1&sn=ad6693cdeaa18034ed1c53271f642ef7&chksm=871b0134b06c8822bf89781a81081c161eb82b06d0c20b655bd7b991202d363b6c233ef137ff&scene=0&pass_ticket=lD2bnuoAxxbEWgy8KxGVnWVzLL%2FeiSX9MsE68ZdaZQzVoXKXHlCJQ3sVCfTnR7MQ#rd)
2. **augmented-rnns**：google大脑的研究员在博客中讲述了Neural Turing Machine、Attentional Interfaces、Adaptive Computation Time和Neural Programmers四大部分。[英文原文](http://distill.pub/2016/augmented-rnns/)；[新智元翻译版](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2651986905&idx=4&sn=dcfdeb7c92826c0603569d5a86025536&chksm=f1216f28c656e63e309d7c92fd06a1c67ac96ebea2a8c6f90169cd944876fb367a8bf819b4f4&mpshare=1&scene=1&srcid=1002ho2GSC2PTnFhFUio3EYj&pass_ticket=DoiMlYDlmCK%2FTS99n6JzBzzsHdN7QoyC81j%2BvUNHFkqqmuADrJsZlH0yXSTgpVEB#rd)；gitbub博客代码：[https://github.com/distillpub/post--augmented-rnns](https://github.com/distillpub/post--augmented-rnns)
 
### GAN
 
1. **GAN简介**：[《Deep Learning Research Review Week 1: Generative Adversarial Nets》](https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-1-Generative-Adversarial-Nets)
2. **生成式对抗网络GAN研究进展系列笔记**：[http://blog.csdn.net/solomon1558/article/details/52537114](http://blog.csdn.net/solomon1558/article/details/52537114)
 
### Reinforcement Learning
 
1. **强化学习概览**：NVIDIA 博客上 Tim Dettmers 所写的《Deep Learning in a Nutshell》系列文章的第四篇：[强化学习概览](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650719294&idx=1&sn=f1a01cd6710e6ea9629619cd3324d102&chksm=871b0040b06c895642ff961a6fe81f05c5e9776aff5da4845f2d3d874f88213863afd2059833&mpshare=1&scene=1&srcid=1002mDtDsEDixxCswQJOs2rH&pass_ticket=DoiMlYDlmCK%2FTS99n6JzBzzsHdN7QoyC81j%2BvUNHFkqqmuADrJsZlH0yXSTgpVEB#rd)
 
# **2. Useful Tools**
 
## Datasets
 
1. **Youtube-8m**：该数据集包含了 800 万个 YouTube 视频 URL（代表着 500,000 小时的视频）以及它们的视频层面的标签（video-level labels），这些标签来自一个多样化的包含 4800 个知识图谱实体（Knowledge Graph entity）的集合。相比于之前已有的视频数据集，这个数据集的规模和多样性都实现了显著的增长。比如说，我们所知的之前最大的视频数据集 Sports-1M 包含了大约 100 万段 YouTube 视频和 500 个体育领域的分类——YouTube-8M 在视频数量和分类数量上都差不多比它高一个数量级。论文：***《YouTube-8M: A Large-Scale Video Classification Benchmark》***
2. **Open Images(图片数据集，包含9百万标注图片)**:一个包含了900万图像URL的数据集，值得一提的是，这些图像全部都是标签数据，标签种类超过6000种。我们尽量让数据集变得实用：数据集中所使用的标签类型比拥有1000个分类的ImageNet数据集更加贴近实际生活。 [https://github.com/openimages/dataset](https://github.com/openimages/dataset)

## pretrained models

1. **大规模语言建模模型库(基于One Billion Word Benchmark)**：这个数据库含有大约 10 亿个单词，词汇有 80 万单词，大部分都是新闻数据。由于训练中句子是被打乱了的，模型可以不理会文本，集中句子层面的语言建模。在此基础上，作者在论文描述了一个模型，混合了字符CNN（character CNN）、大规模深度 LSTM，以及一个专门的 Softmanx 架构，最终得到的结果可以说是迄今最好的。github:[https://github.com/tensorflow/models/tree/master/lm_1b](https://github.com/tensorflow/models/tree/master/lm_1b)
 
## Deep Learning Tools
 
### mxnet
1. **NNVM和tinyflow**：[《NNVM打造模块化深度学习系统》](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650719529&idx=3&sn=6992a6067c79349583762cb28eecda89&chksm=871b0157b06c8841587bdfb992c19290c8d66386a6f8accdf70998ce3f86b36330219c09672d&scene=0&pass_ticket=lD2bnuoAxxbEWgy8KxGVnWVzLL%2FeiSX9MsE68ZdaZQzVoXKXHlCJQ3sVCfTnR7MQ#rd)前端把计算表达成一个中间形式，通常我们称之为计算图，NNVM 则统一的对图做必要的操作和优化，然后再生成后端硬件代码。NNVM 是一个神经网络的比较高级的中间表示模块，它包含了图的表示以及执行无关的各种优化（例如内存分配，数据类型和形状的推导）。核心的是这两个github地址：[https://github.com/dmlc/nnvm](https://github.com/dmlc/nnvm)和[https://github.com/tqchen/tinyflow](https://github.com/tqchen/tinyflow)。

### theano
1. **bay area dl school's tutorial**：[https://github.com/daiwk/bayareadlschool-learning-theano](https://github.com/daiwk/bayareadlschool-learning-theano)

### torch
1. **bay area dl school's tutorial**：[https://github.com/daiwk/bayareadlschool-learning-torch](https://github.com/daiwk/bayareadlschool-learning-torch)

### tensorflow
1. **bay area dl school's tutorial**：[https://github.com/daiwk/bayareadlschool-learning-tensorflow](https://github.com/daiwk/bayareadlschool-learning-tensorflow)

 
# **3. Useful Courses && Speeches**
 
### Courses
1. **cs224d(nlp)**：课程链接：[http://cs224d.stanford.edu/syllabus.html](http://cs224d.stanford.edu/syllabus.html)；百度云课程pdf下载：[http://pan.baidu.com/s/1dFaA7PR](http://pan.baidu.com/s/1dFaA7PR)
2. **cs231n(cnn)**：课程链接：[http://cs231n.stanford.edu/](http://cs231n.stanford.edu/)
3. **Bay Area Deep Learning School 2016**：
课程安排（附课件下载链接）：
[http://www.bayareadlschool.org/schedule](http://www.bayareadlschool.org/schedule)

演讲视频：

day1: youtube: [https://www.youtube.com/watch?v=eyovmAtoUx0](https://www.youtube.com/watch?v=eyovmAtoUx0); 优酷：[http://v.youku.com/v_show/id_XMTczNzYxNjg5Ng==.html](http://v.youku.com/v_show/id_XMTczNzYxNjg5Ng==.html)

day2：youtube：[https://www.youtube.com/watch?v=eyovmAtoUx0](https://www.youtube.com/watch?v=eyovmAtoUx0)；优酷：[http://v.youku.com/v_show/id_XMTczODc2ODE3Mg==.html](http://v.youku.com/v_show/id_XMTczODc2ODE3Mg==.html)

daiwk整理: [deep reinforcement learning](http://pan.baidu.com/s/1mim7A2G)；[deep unsupervised learning](http://pan.baidu.com/s/1o8tVcue)

小结：[Yoshua Bengio压轴解读深度学习的基础和挑战](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650719442&idx=1&sn=ff9f8412f08dbb8e52cb1fdb748e5a4e&chksm=871b00acb06c89ba46582fe5c481a5bc93a69cca5e1eb9f0d86d03db99cb9db3b1c8fdaabde4&mpshare=1&scene=1&srcid=092660FGFS96T6aIXG9i1pI0&pass_ticket=DoiMlYDlmCK%2FTS99n6JzBzzsHdN7QoyC81j%2BvUNHFkqqmuADrJsZlH0yXSTgpVEB#rd)；[Andrej Karpathy 最新演讲计算机视觉深度学习技术与趋势](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2651987425&idx=1&sn=e5d4bb50352bf536d786bacb4cb16258&chksm=f1216910c656e006cdc2de9aa4cdf19b82f4dd79bb1ede4710f8876056381655e339a57b0bea&mpshare=1&scene=1&srcid=1002SLviFNa9PO6AXaJghl00&pass_ticket=DoiMlYDlmCK%2FTS99n6JzBzzsHdN7QoyC81j%2BvUNHFkqqmuADrJsZlH0yXSTgpVEB#rd)；

### Speeches
1. **google brain最近的7大研究项目**：[《谷歌大脑最近7大研究项目》](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2651987619&idx=1&sn=3f24b3384a9b10fce2001f4074b789ee&chksm=f1216a52c656e344b1f449f384fe0f41461c9e72992946a1b79c2d1d996eb4bef73e27e04f6c&scene=0&pass_ticket=lD2bnuoAxxbEWgy8KxGVnWVzLL%2FeiSX9MsE68ZdaZQzVoXKXHlCJQ3sVCfTnR7MQ#rd)


# 4. **Applications**
 
## NLP

### Text Abstraction
1. **Abstractive Text Summarization using Seq-to-Seq RNNs and Beyond**：来自IBM Watson，本文是一篇非常优秀的paper，在seq2seq+attention的基础上融合了很多的features、trick进来，提出了多组对比的模型，并且在多种不同类型的数据集上做了评测，都证明了本文模型更加出色。[张俊的分析](https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247483777&idx=1&sn=d766a3dec1761bab4186cf89ce8a8723&mpshare=1&scene=1&srcid=0919dvXyKB1zjz6S543FH1Ib&pass_ticket=DoiMlYDlmCK%2FTS99n6JzBzzsHdN7QoyC81j%2BvUNHFkqqmuADrJsZlH0yXSTgpVEB#rd)
