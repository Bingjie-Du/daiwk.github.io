---
layout: post
category: "video"
title: "tsm"
tags: [tsm, ]
---

目录

<!-- TOC -->

- [TSM](#tsm)
- [加速](#%e5%8a%a0%e9%80%9f)

<!-- /TOC -->


## TSM

ICCV2019上，[TSM: Temporal Shift Module for Efficient Video Understanding](https://arxiv.org/abs/1811.08383)

## 加速

[15分钟完成Kinetics视频识别训练，除了超级计算机你还需要TSM](https://mp.weixin.qq.com/s/a_cXMashqCrD-b65rusTLQ)

[Training Kinetics in 15 Minutes: Large-scale Distributed Training on Videos](https://arxiv.org/abs/1910.00932)

[https://github.com/mit-han-lab/temporal-shift-module](https://github.com/mit-han-lab/temporal-shift-module)

[https://hanlab.mit.edu/projects/tsm/](https://hanlab.mit.edu/projects/tsm/)

深度视频识别的计算成本比图像识别更高，尤其是在 Kinetics 等大规模数据集上。因此，为了处理大量视频，可扩展性训练是至关重要的。这篇论文研究了影响视频网络的可扩展性的因素。研究者认定了三个瓶颈，包括数据加载（从磁盘向 GPU 移动数据）、通信（在网络中移动数据）和计算速度（FLOPs）。

针对这些瓶颈，研究者提出了三种可以提升可扩展性的设计原则：（1）使用 FLOPs 更低且对硬件友好的算子来提升计算效率；（2）降低输入帧数以减少数据移动和提升数据加载效率，（3）减少模型大小以降低网络流量和提升网络效率。

基于这些原则，研究者设计了一种新型的算子「时间位移模块（TSM：Temporal Shift Module）」，能够实现高效且可扩展的分布式训练。相比于之前的 I3D 模型，TSM 模型的吞吐量可以高出 1.8 倍。

研究者也通过实验测试了新提出的 TSM 模型。将 TSM 模型的训练扩展到了 1536 个 GPU 上，使用了包含 12288 个视频片段/ 98304 张图像的 minibatch，没有造成准确度损失。使用这样的硬件友好的模型设计，研究者成功地扩展了在 Summit 超级计算机上的训练，将在 Kinetics 数据集上的训练时间从 49 小时 55 分减少到了 14 分 13 秒，同时实现了 74.0% 的 top-1 准确度，这在准确度更高的同时还比之前的 I3D 视频模型快 1.6 和 2.9 倍。

在计算机视觉领域，视频识别是一个至关重要的分支。视频识别问题的难度更高，但得到的研究更少：（1）相比于 2D 图像模型，视频模型的计算成本通常高一个数量级。举个例子，很常见的 ResNet-50 模型的速度大约是 4G FLOPs，而 ResNet-50 I3D 则要消耗 33G FLOPs，多过 8 倍；（2）视频数据集比 2D 图像数据集大得多，而且数据 I/O 也比图像高很多。举个例子，ImageNet 有 128 万张训练图像，而视频数据集 Kinetics-400 有 6300 万训练帧，大约是前者的 50 倍；（3）视频模型的模型大小通常更大，因此需要更高的网络带宽来交换梯度。
