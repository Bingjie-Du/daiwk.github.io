---
layout: post
category: "nlp"
title: "Deep contextualized word representations(ELMo)"
tags: [elmo, ]
---

目录

<!-- TOC -->


<!-- /TOC -->

[https://zhuanlan.zhihu.com/p/37915351](https://zhuanlan.zhihu.com/p/37915351)

[Deep contextualized word representations](https://arxiv.org/abs/1802.05365)

之前的glove以及word2vec的word embedding在nlp任务中都取得了最好的效果, 现在几乎没有一个NLP的任务中不加word embedding.

我们常用的获取embedding方法都是通过训练language model, 将language model中预测的hidden state做为word的表示, 给定N个tokens的序列`\((t_1, t_2,...,t_n)\)`, 前向language model就是通过前k-1个输入序列`\((t_1, t_2, ...,t_k)\)`的hidden表示, 预测第k个位置的token, 反向的language model就是给定后面的序列, 预测之前的, 然后将language model的第k个位置的hidden输出做为word embedding.

之前的做法的缺点是对于每一个单词都有唯一的一个embedding表示, 而对于多义词显然这种做法不符合直觉, 而单词的意思又和上下文相关, ELMo的做法是我们只预训练language model, 而word embedding是通过输入的句子实时输出的, 这样单词的意思就是上下文相关的了, 这样就很大程度上缓解了歧义的发生. 且ELMo输出多个层的embedding表示, 试验中已经发现每层LM输出的信息对于不同的任务效果不同, 因此对每个token用不同层的embedding表示会提升效果.

