---
layout: post
category: "ml"
title: "机器学习相关tips"
tags: [ml, tips]
---


## 1. 评价指标

### 1.1 关于auc和p/r/f1...

参考[https://stackoverflow.com/questions/34698161/how-to-interpret-almost-perfect-accuracy-and-auc-roc-but-zero-f1-score-precisio](https://stackoverflow.com/questions/34698161/how-to-interpret-almost-perfect-accuracy-and-auc-roc-but-zero-f1-score-precisio)

One must understand crucial difference between AUC ROC and "point-wise" metrics like accuracy/precision etc. ROC is a function of a threshold. Given a model (classifier) that outputs the probability of belonging to each class we usually classify element to the class with the highest support. However, sometimes we can get better scores by changing this rule and requiring one support to be 2 times bigger than the other to actually classify as a given class. This is often true for imbalanced datasets. This way you are actually modifing the learned prior of classes to better fit your data. ROC looks at **"what would happen if I change this threshold to all possible values"** and then AUC ROC computes the integral of such a curve.

Consequently:

+ **high AUC ROC vs low f1 or other "point" metric,** means that your classifier currently does a bad job, however you can find the threshold for which its score is actually pretty decent
+ **low AUC ROC and low f1 or other "point" metric,** means that your classifier currently does a bad job, and even fitting a threshold will not change it
+ **high AUC ROC and high f1 or other "point" metric,** means that your classifier currently does a decent job, and for many other values of threshold it would do the same
+ **low AUC ROC vs high f1 or other "point" metric,** means that your classifier currently does a decent job, however for many other values of threshold - it is pretty bad

## 1.2 roc curve v.s. precision-recall curve

[https://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves](https://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves)