---
layout: post
category: "ml"
title: "ftrl"
tags: [ftrl, ]
---

目录

<!-- TOC -->

- [google的几篇重要paper](#google的几篇重要paper)
    - [Adaptive Bound Optimization for Online Convex Optimization](#adaptive-bound-optimization-for-online-convex-optimization)
    - [A Unified View of Regularized Dual Averaging and Mirror Descent with Implicit Updates](#a-unified-view-of-regularized-dual-averaging-and-mirror-descent-with-implicit-updates)
    - [Follow-the-Regularized-Leader and Mirror Descent: Equivalence Theorems and L1 Regularization](#follow-the-regularized-leader-and-mirror-descent-equivalence-theorems-and-l1-regularization)
    - [Ad Click Prediction: a View from the Trenches](#ad-click-prediction-a-view-from-the-trenches)
- [背景](#背景)
    - [问题描述](#问题描述)

<!-- /TOC -->

参考[http://www.cnblogs.com/EE-NovRain/p/3810737.html](http://www.cnblogs.com/EE-NovRain/p/3810737.html)

## google的几篇重要paper

### Adaptive Bound Optimization for Online Convex Optimization

[Adaptive Bound Optimization for Online Convex Optimization](https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/36483.pdf)

2010年的理论性的paper，但未显式地支持正则化项迭代。

### A Unified View of Regularized Dual Averaging and Mirror Descent with Implicit Updates

[A Unified View of Regularized Dual Averaging and Mirror Descent with Implicit Updates](https://pdfs.semanticscholar.org/50eb/06a0e58962715393d7adc26318b54521db9b.pdf)

11年的paper，证明了regret bound以及引入通用的正则化项

### Follow-the-Regularized-Leader and Mirror Descent: Equivalence Theorems and L1 Regularization

[Follow-the-Regularized-Leader and Mirror Descent: Equivalence Theorems and L1 Regularization](https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/37013.pdf)

11年，FOBOS、RDA、FTRL等各种方法对比的paper

高度概括呢（参考开篇说到的那篇博客），就是最小化的loss有三项：

<html>
<br/>
<img src='../assets/ftrl-and-other.png' style='max-height: 300px'/>
<br/>
</html>

+ 第一项：梯度或累积梯度；
+ 第二项：L1正则化项的处理(注：`\(\Psi(x)=\|x\|_{1}\)`)；
+ 第三项：这个累积加和限定了新的迭代结果x**不要离已迭代过的解太远**（也即FTRL-Proximal中**proximal**的含义），或者**离0太远（central）**，这一项其实也是**low regret**的需求

### Ad Click Prediction: a View from the Trenches

把ftrl用在ctr预估上的工程性的论文：[Ad Click Prediction: a View from the Trenches](http://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf)


## 背景

传统的批量（batch）算法无法有效地处理超大规模的数据集和在线数据流，FTRL（Follow-the-regularized-Leader）在处理诸如逻辑回归之类的带非光滑正则化项（例如1范数，做模型复杂度控制和稀疏化）的凸优化问题上性能非常出色。

### 问题描述

对于loss函数+正则化的结构风险最小化的优化问题（逻辑回归也是这种形式）有两种等价的描述形式，以1范数为例

+ 无约束优化形式的soft regularization formulation

`\[
\hat{w}=\underset{w}{\textrm{argmin}}\sum_{i=1}^{n}L(w,z_i)+g||w||_1
\]`

+ 带约束项的凸优化问题convex constraint formulation：

`\[
\hat{w}=\underset{w}{\textrm{argmin}}\sum_{i=1}^{n}L(w,z_i), s.t.||w||_1\leq s
\]`
