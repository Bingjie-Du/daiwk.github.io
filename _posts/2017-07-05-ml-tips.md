---
layout: post
category: "ml"
title: "机器学习相关tips"
tags: [ml, tips]
---


## 1. 评价指标

### 1.1 关于auc和p/r/f1...

参考[https://stackoverflow.com/questions/34698161/how-to-interpret-almost-perfect-accuracy-and-auc-roc-but-zero-f1-score-precisio](https://stackoverflow.com/questions/34698161/how-to-interpret-almost-perfect-accuracy-and-auc-roc-but-zero-f1-score-precisio)

One must understand crucial difference between AUC ROC and "point-wise" metrics like accuracy/precision etc. ROC is a function of a threshold. Given a model (classifier) that outputs the probability of belonging to each class we usually classify element to the class with the highest support. However, sometimes we can get better scores by changing this rule and requiring one support to be 2 times bigger than the other to actually classify as a given class. This is often true for imbalanced datasets. This way you are actually modifing the learned prior of classes to better fit your data. ROC looks at **"what would happen if I change this threshold to all possible values"** and then AUC ROC computes the integral of such a curve.

Consequently:

+ **high AUC ROC vs low f1 or other "point" metric,** means that your classifier currently does a bad job, however you can find the threshold for which its score is actually pretty decent
+ **low AUC ROC and low f1 or other "point" metric,** means that your classifier currently does a bad job, and even fitting a threshold will not change it
+ **high AUC ROC and high f1 or other "point" metric,** means that your classifier currently does a decent job, and for many other values of threshold it would do the same
+ **low AUC ROC vs high f1 or other "point" metric,** means that your classifier currently does a decent job, however for many other values of threshold - it is pretty bad

## 1.2 roc curve v.s. precision-recall curve

[https://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves](https://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves)

既然已经这么多评价标准，为什么还要使用ROC和AUC呢？因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现类不平衡（class imbalance）现象，即负样本比正样本多很多（或者相反），而且测试数据中的正负样本的分布也可能随着时间变化。下图是ROC曲线和Precision-Recall曲线[(The relationship between Precision-Recall and ROC curves)](http://mark.goadrich.com/articles/davisgoadrichcamera2.pdf)的对比：



<html>
<br/>
<img src='../assets/roc_vs_pr_curve.jpg' style='max-height: 300px'/>
<br/>
</html>

在上图中，(a)和(c)为ROC曲线，(b)和(d)为Precision-Recall曲线。(a)和(b)展示的是分类其在原始测试集（正负样本分布平衡）的结果，(c)和(d)是将测试集中负样本的数量增加到原来的10倍后，分类器的结果。可以明显的看出，ROC曲线基本保持原貌，而Precision-Recall曲线则变化较大。