---
layout: post
category: "platform"
title: "美图的MML"
tags: [美图, MML, bamboo, ]
---

目录

<!-- TOC -->


<!-- /TOC -->

[当推荐遇到社交：美图的推荐算法设计优化实践](https://mp.weixin.qq.com/s/Eih4J51C8Eh-cuZ8vznESg)

MML 机器学习平台包括三个主要模块：

+ Spark Feature：负责数据分析、特征工程，以及样本拼接。Spark Feature 基于 Spark SQL 进行开发，用户通过编写 SQL 以及配置样本拼接 JSON，即可实现特征以及样本生产的工作；
+ Bamboo：基于 tensorflow 开发，负责模型训练、离线效果评估。Bamboo 实现了推荐领域大量的 State of the Art 的模型，并且提供了丰富的 Layers，以简化算法同学的建模工作。在训练方面支持多种并行训练方式，同时通过对代码的优化实现了较高的训练效率；
+ MML Serving：负责模型的在线服务。底层通过 C++ 实现，在内存和并发上做了大量的优化，支持同时请求多个模型，以及在线热更。灵活的架构让我们能够很方便地接入各种机器学习框架训练的模型。

Bamboo 具有以下优点：

+ 便捷：内置了近几年推荐领域的 SOTA 模型，以及建模常用的 Layers，并且内置了部分公共数据集的访问接口，能够支持从本地磁盘，以及 HDFS 读取训练数据。数据、训练、模型评估、模型导出通过配置化实现，算法同学可以专注于模型的设计；
+ 高效：采用 tensorflow 底层 API 和 Estimator 来实现，并遵循 tensorflow 官方性能优化指南，最大限度提升模型训练效率，相比 Keras 以及内部未优化版本，单卡训练效率有数倍提升。同时，能够支持同步、异步等多种并行训练方案；
+ 可扩展：Bamboo 的最初的设计目标是作为 tensorflow 的补充，因此在整个设计过程充分考虑了扩展性，能够支持采用 Bamboo 提供的 API 或者使用 tensorflow 原生 API。良好的分层设计，方便使用方进行模块的复用和重构。

MML Serving 决定了模型能否上线提供服务以及在线服务的效率。去年下半年，我们上线了采用 C++ 开发的新版 MML Serving，通过内存和并发的优化，让我们整体预估耗时减少了 50%，服务初始化耗时减少了 50%，内存使用量降低了 77%。通过压测发现，服务在高并发下，整体表现稳定。另外良好的架构设计，可以很方便接入各种第三方机器学习库，目前已经内置了对 tensorflow 和 xgboost 模型的支持。

2018 年，我们上线了第一个基于何向南在 SIGIR 2017 发表的《Neural Factorization Machines for Sparse Predictive Analytics》改进的模型——NFM-v4。相比原论文，我们的主要改进点是通过一个线性变换，将变长稀疏的原始高维特征压缩到一个定长稠密的低维实数空间，从而屏蔽了模型在输入特征处理上的差异，可以将精力更多放在特征的挖掘上。

但是，将几十万维的高维空间直接压缩到几百维，存在一定的信息损失，因此，在 NFM-v4 的基础上，我们通过将部分高维 id 特征单独建模，比较好的解决了这个问题，在业务指标上，也有不错的效果提升，美拍的人均播放时长增加了 4.75%，人均有效行为数增加了 3.45%。

不过，NFM 存在的一个问题是，bi-interaction pooling 认为特征二阶交叉的权重是相等的，这种假设在多数场景下并不符合数据的真实分布。因此，在 NFM 的基础上，我们提出了 Neural Field weighted Factorization Machines（NFwFM）模型，通过引入一个权重向量，来建模二阶交叉特征的权重。通过二阶向量不等权相加，业务指标整体提升较为明显。其中美拍人均播放时长增加 3.78%，播放用户数增加 1.74%，美图秀秀点击率提升了 5.689%，人均使用时长增加 2.53%，新用户点击率增加 2.701%。

