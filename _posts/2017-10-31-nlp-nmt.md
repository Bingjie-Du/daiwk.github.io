---
layout: post
category: "nlp"
title: "nmt"
tags: [nmt, ]
---

## GRU

GRU是Cho等人在LSTM上提出的简化版本，也是RNN的一种扩展，如下图所示。GRU单元只有两个门：

+ 重置门（reset gate）：如果重置门关闭，会忽略掉历史信息，即历史不相干的信息不会影响未来的输出。
+ 更新门（update gate）：将LSTM的输入门和遗忘门合并，用于控制历史信息对当前时刻隐层输出的影响。如果更新门接近1，会把历史信息传递下去。

<html>
<br/>

<img src='../assets/gru.png' style='max-height: 350px;max-width:500px'/>
<br/>

</html>

我们已经在语义角色标注一章中介绍了一种双向循环神经网络，这里介绍Bengio团队在论文[2,4]中提出的另一种结构。该结构的目的是输入一个序列，得到其在每个时刻的特征表示，即输出的每个时刻都用定长向量表示到该时刻的上下文语义信息。

具体来说，该双向循环神经网络分别在时间维以顺序和逆序——即前向（forward）和后向（backward）——依次处理输入序列，并将每个时间步RNN的输出拼接成为最终的输出层。这样每个时间步的输出节点，都包含了输入序列中当前时刻完整的过去和未来的上下文信息。下图展示的是一个按时间步展开的双向循环神经网络。该网络包含一个前向和一个后向RNN，其中有六个权重矩阵：输入到前向隐层和后向隐层的权重矩阵（W1,W3），隐层到隐层自己的权重矩阵（W2,W5），前向隐层和后向隐层到输出层的权重矩阵（W4,W6）。注意，该网络的前向隐层和后向隐层之间没有连接。

<html>
<br/>

<img src='../assets/bi-lstm-bengio.png' style='max-height: 350px;max-width:500px'/>
<br/>

</html>

## encoder-decoder框架

编码器-解码器（Encoder-Decoder）([Learning phrase representations using RNN encoder-decoder for statistical machine translation](http://www.aclweb.org/anthology/D/D14/D14-1179.pdf))框架用于解决由一个任意长度的源序列到另一个任意长度的目标序列的变换问题。即编码阶段将整个源序列编码成一个向量，解码阶段通过最大化预测序列概率，从中解码出整个目标序列。编码和解码的过程通常都使用RNNN实现。

<html>
<br/>

<img src='../assets/encoder-decoder.png' style='max-height: 350px;max-width:500px'/>
<br/>

</html>


### encoder

总体流程如下：
<html>
<br/>

<img src='../assets/encoder-process.png' style='max-height: 350px;max-width:600px'/>
<br/>

</html>


<html>
<br/>

<img src='../assets/encoder.png' style='max-height: 350px;max-width:500px'/>
<br/>

</html>

### decoder

<html>
<br/>

<img src='../assets/decoder-process.png' style='max-height: 350px;max-width:600px'/>
<br/>

</html>



<html>
<br/>

<img src='../assets/decoder-attention.png' style='max-height: 350px;max-width:500px'/>
<br/>

</html>


