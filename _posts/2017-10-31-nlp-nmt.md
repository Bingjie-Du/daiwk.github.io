---
layout: post
category: "nlp"
title: "nmt"
tags: [nmt, ]
---

目录

<!-- TOC -->

- [1. GRU](#1-gru)
- [2. 双向GRU](#2-双向gru)
- [3. encoder-decoder框架](#3-encoder-decoder框架)
    - [3.1 encoder](#31-encoder)
    - [3.2 decoder](#32-decoder)
- [4. 注意力机制](#4-注意力机制)
- [5. beam search](#5-beam-search)
- [6. paddle demo](#6-paddle-demo)
    - [6.1 模型结构](#61-模型结构)
        - [6.1.1 编码器](#611-编码器)
        - [6.1.2 解码器](#612-解码器)
    - [6.2 训练模型](#62-训练模型)
    - [6.3 生成模型](#63-生成模型)
- [7. haha](#7-haha)

<!-- /TOC -->

paddle rnn系列文档：
[https://github.com/PaddlePaddle/Paddle/tree/develop/doc/howto/deep_model/rnn ](https://github.com/PaddlePaddle/Paddle/tree/develop/doc/howto/deep_model/rnn )

## 1. GRU

GRU是Cho等人在LSTM上提出的简化版本，也是RNN的一种扩展，如下图所示。GRU单元只有两个门：

+ 重置门（reset gate）：如果重置门关闭，会忽略掉历史信息，即历史不相干的信息不会影响未来的输出。
+ 更新门（update gate）：将LSTM的输入门和遗忘门合并，用于控制历史信息对当前时刻隐层输出的影响。如果更新门接近1，会把历史信息传递下去。

`\(z_t\)`【更新门】和`\(r_t\)`【重置门】都作用(sigmoid)于`\([h_{t-1},x_t]\)`【即上一时刻的隐层状态`\(h_{t-1}\)`和这一时刻的输入`\(x_t\)`】。

而重置门作用于`\(h_{t-1}\)`得到`\(r_t*h_{t-1}\)`【注意，是element-wise乘积】再与`\(x_t\)`一起经过tanh得到节点状态。

最终的输出是更新门与节点状态相乘，再加上`\(1-z_t\)`与上一时刻的隐层状态`\(h_{t-1}\)`的乘积。

总结一下：

+ 更新门和重置门都作用于上一时刻的隐层状态和这一时刻的输入。
+ 节点状态是本时刻的输入与经过重置门作用了的上一时刻的输出一起决定的。
+ 最终输出是更新门作用于节点状态，1-更新门作用于上一时刻隐层状态。



<html>
<br/>

<img src='../assets/gru.png' style='max-height: 350px;max-width:500px'/>
<br/>

</html>

## 2. 双向GRU

我们已经在语义角色标注一章中介绍了一种双向循环神经网络，这里介绍Bengio团队在论文【Bahdanau等人[Neural Machine Translation by Jointly Learning to Align and Translate, ICLR, 2015](https://arxiv.org/pdf/1409.0473.pdf)】中提出的另一种结构。该结构的目的是输入一个序列，得到其在每个时刻的特征表示，即输出的每个时刻都用定长向量表示到该时刻的上下文语义信息。

具体来说，该双向循环神经网络分别在时间维以顺序和逆序——即前向（forward）和后向（backward）——依次处理输入序列，并将每个时间步RNN的输出拼接成为最终的输出层。这样每个时间步的输出节点，都包含了输入序列中当前时刻完整的过去和未来的上下文信息。下图展示的是一个按时间步展开的双向循环神经网络。该网络包含一个前向和一个后向RNN，其中有六个权重矩阵：输入到前向隐层和后向隐层的权重矩阵（W1,W3），隐层到隐层自己的权重矩阵（W2,W5），前向隐层和后向隐层到输出层的权重矩阵（W4,W6）。注意，该网络的前向隐层和后向隐层之间没有连接。

<html>
<br/>

<img src='../assets/bi-lstm-bengio.png' style='max-height: 350px;max-width:500px'/>
<br/>

</html>

## 3. encoder-decoder框架

编码器-解码器（Encoder-Decoder）([Learning phrase representations using RNN encoder-decoder for statistical machine translation](http://www.aclweb.org/anthology/D/D14/D14-1179.pdf))框架用于解决由一个任意长度的源序列到另一个任意长度的目标序列的变换问题。即编码阶段将整个源序列编码成一个向量，解码阶段通过最大化预测序列概率，从中解码出整个目标序列。编码和解码的过程通常都使用RNN实现。

<html>
<br/>

<img src='../assets/encoder-decoder.png' style='max-height: 350px;max-width:500px'/>
<br/>

</html>

### 3.1 encoder

总体流程如下：
<html>
<br/>

<img src='../assets/encoder-process.png' style='max-height: 350px;max-width:600px'/>
<br/>

</html>

重点在于：
最后对于词xi，通过拼接两个GRU的结果得到它的隐层状态。

<html>
<br/>

<img src='../assets/encoder.png' style='max-height: 350px;max-width:500px'/>
<br/>

</html>

### 3.2 decoder

<html>
<br/>

<img src='../assets/decoder-process.png' style='max-height: 350px;max-width:600px'/>
<br/>

</html>

关键在于，预测的`\(z_{i+1}\)`由以下三部分经过**非线性激活**产生：
+ 源语言句子的**编码信息**`\(c\)`(`\(c=qh\)`，如果不用注意力，可以直接取最后一个时间步的编码`\(c=h_T\)`，也可以用时间维上的pooling的结果)。
+ **真实目标语言序列**的第i个词`\(u_i\)`。`\(u_0\)`是开始标志```<s>```。
+ i时刻的RNN**隐层状态**`\(z_i\)`, `\(z_0\)`是全零向量。

## 4. 注意力机制

如果在编码阶段的输出是一个固定维度的向量，会有以下两个问题：

+ 不论源语言序列长度是5个词还是50个词，如果都用固定长度的向量去编码其中的语义和句法信息，对模型来讲，**是一个非常高的要求，特别对长句子而言。**
+ 直觉上，当人类翻译一句话时，会对**与【当前】译文更相关的源语言片段给予更多关注，且关注点会随翻译的进行而改变。**

而固定维度的向量相当于，任何时刻都对源语言所有信息给予同等程度的关注，Bahdanau等人[Neural Machine Translation by Jointly Learning to Align and Translate, ICLR, 2015](https://arxiv.org/pdf/1409.0473.pdf)引入注意力机制，对编码后的上下文片段进行解码，以此解决长句子的特征学习问题。

和简单的编码器不同之处在于，前面生成`\(z_{i+1}\)`用的是`\(c\)`，而这里用的是`\(c_i\)`，也就是，对每个真实目标语言序列中的词`\(u_i\)`【注意！！是目标语言的i，不是源语言的！！】，都有一个特定的`\(c_i\)`与之对应。

`\[
z_{i+1}=\phi _{\theta }(c_i, u_i, z_i)
\]`

`\[
c_i=\sum ^T_{j=1}a_{ij}h_j, a_i=[a_{i1}, a_{i2}, ..., a_{iT}]
\]`

可见，注意力机制是通过**对编码器中各时刻的RNN状态**`\(h_j\)`进行加权平均实现的。权重`\(a_{ij}\)`计算方法如下：

`\[
c_i=\sum ^T_{j=1}a_{ij}h_j, a_i=[a_{i1}, a_{i2}, ..., a_{iT}]
\]`

`\[
a_{ij}=\frac {exp(e_{ij})}{\sum _{k=1}^Texp(e_{ik})}
\]`

`\[\
e_{ij}=align(z_i,h_j)
\]`

其中，align可以看作一个对齐模型，用于衡量**目标语言第i个词**（第i个隐层状态`\(z_i\)`）和**源语言第j个词**（第j个词的上下文片段`\(h_j\)`）的匹配程度。

传统的对齐模型中，目标语言的每个词明确对应源语言的一个词或多个词（hard alignment）；而这里用的是soft alignment，即**任何两个目标语言和源语言词间**均存在一定的关联（模型计算出的实数值）。

<html>
<br/>

<img src='../assets/decoder-attention.png' style='max-height: 350px;max-width:500px'/>
<br/>

</html>

## 5. beam search

柱搜索（beam search）算法是一种启发式图搜索算法，用于在图或树中搜索有限集合中的最优扩展节点。通常用在**解空间非常大**的系统（如机器翻译、语音识别）中，因为内存无法存下图或权中所有展开的解。

beam search使用广度优先策略建立搜索树，在树的每一层，按照启发代价（heuristic cost, **例如本例中的生成词的log概率之和**）对节点进行排序，然后仅留下预先确定的个数(即beam width/beam size/柱宽度)的节点。只有这些节点会在下一层继续进行扩展，其他节点被裁剪掉了。可以减少搜索所占用的时间和空间，但无法保证一定获得最优解。

使用beam search的解码阶段，目标是最大化生成序列的概率，具体思路如下：

+ 每个时刻，根据源语言句子的编码信息c、生成的第i个真实目标语言序列单词`\(u_i\)`，和i时刻RNN的隐层状态`\(z_i\)`，计算出下一个隐层状态`\(z_{i+1}\)`
+ 将`\(z_{i+1}\)`通过softmax【只针对beamsearch得到的可能的解做处理】归一化，得到的目标语言序列的第i+1个单词的概率分布`\(p_{i+1}\)`
`\[
p(u_{i+1}|u_{\lt i+1}, x)=softmax(W_sz_{i+1}+b_z)
\]`
其中，`\(W_sz_{i+1}+b_z\)`是**对每个可能的输出单词进行打分**
+ 根据`\(p_{i+1}\)`采样出单词`u_{i+1}`
+ 重复前三个步骤，直到**获得句子的结束标记```<e>```或者超过句子的最大生成长度为止。**

## 6. paddle demo

### 6.1 模型结构

一些全局变量

```python
dict_size = 30000 # 字典维度
source_dict_dim = dict_size # 源语言字典维度
target_dict_dim = dict_size # 目标语言字典维度
word_vector_dim = 512 # 词向量维度
encoder_size = 512 # 编码器中的GRU隐层大小
decoder_size = 512 # 解码器中的GRU隐层大小
beam_size = 3 # 柱宽度
max_length = 250 # 生成句子的最大长度
```

#### 6.1.1 编码器

输入是一个文字序列，被表示成整型的序列。序列中每个元素是文字在字典中的索引

```python
src_word_id = paddle.layer.data(
        name='source_language_word',
        type=paddle.data_type.integer_value_sequence(source_dict_dim))
```

#### 6.1.2 解码器


### 6.2 训练模型

### 6.3 生成模型


## 7. haha
