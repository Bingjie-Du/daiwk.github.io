---
layout: post
category: "dl"
title: "multi-task learning"
tags: [mtl, multi-task learning, 多目标, ]
---

目录

<!-- TOC -->

- [两大类mtl方法](#%e4%b8%a4%e5%a4%a7%e7%b1%bbmtl%e6%96%b9%e6%b3%95)
  - [hard参数共享](#hard%e5%8f%82%e6%95%b0%e5%85%b1%e4%ba%ab)
  - [soft参数共享](#soft%e5%8f%82%e6%95%b0%e5%85%b1%e4%ba%ab)
- [mtl为什么会work](#mtl%e4%b8%ba%e4%bb%80%e4%b9%88%e4%bc%9awork)
  - [隐式数据增强](#%e9%9a%90%e5%bc%8f%e6%95%b0%e6%8d%ae%e5%a2%9e%e5%bc%ba)
  - [Attention focusing](#attention-focusing)
  - [Eavesdropping](#eavesdropping)
  - [Representation bias](#representation-bias)
  - [Regularization](#regularization)
- [非神经网络中的mtl](#%e9%9d%9e%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e4%b8%ad%e7%9a%84mtl)
  - [Block-sparse regularization](#block-sparse-regularization)
  - [Learning task relationships](#learning-task-relationships)
- [MTL Deep Learning的Recent work](#mtl-deep-learning%e7%9a%84recent-work)
  - [Deep Relationship Networks](#deep-relationship-networks)
  - [Fully-Adaptive Feature Sharing](#fully-adaptive-feature-sharing)
  - [Cross-stitch Networks](#cross-stitch-networks)
  - [Low supervision](#low-supervision)
  - [A Joint Many-Task model](#a-joint-many-task-model)
  - [Weighting losses with uncertainty](#weighting-losses-with-uncertainty)
  - [Tensor factorisation for MTL](#tensor-factorisation-for-mtl)
  - [Sluice Networks](#sluice-networks)
  - [What should I share in my model?](#what-should-i-share-in-my-model)
- [Auxiliary tasks](#auxiliary-tasks)

<!-- /TOC -->

参考：

+ [http://ruder.io/multi-task/](http://ruder.io/multi-task/)

阿里天池的一个讲解[【深度学习系列09】Multi-Task Learning for E-commerce](https://tianchi.aliyun.com/course/video?spm=5176.12586971.1001.19.55e2194dAHpQl4&liveId=10562)

## 两大类mtl方法

### hard参数共享

所有task共享大的隐层表示，然后每个task有自己的output层。可以极大地减少过拟合，因为大的隐层表示的参数量比每个task自己的小output的参数量要大得多。

<html>
<br/>
<img src='../assets/mtl-hard.png' style='max-height: 200px'/>
<br/>
</html>

### soft参数共享

每个task有自己的模型和自己的参数。模型参数间的距离一般会使用**`\(\ell_2\)`正则**进行正则化，目的是为了让参数间更相似。

<html>
<br/>
<img src='../assets/mtl-soft.png' style='max-height: 200px'/>
<br/>
</html>

## mtl为什么会work

假设task A和task B，有共同的隐层表示F。

### 隐式数据增强

### Attention focusing

### Eavesdropping

### Representation bias

### Regularization

## 非神经网络中的mtl

### Block-sparse regularization

### Learning task relationships

## MTL Deep Learning的Recent work 

### Deep Relationship Networks

### Fully-Adaptive Feature Sharing

### Cross-stitch Networks

### Low supervision

### A Joint Many-Task model

### Weighting losses with uncertainty

### Tensor factorisation for MTL

### Sluice Networks

### What should I share in my model?

## Auxiliary tasks
Related task
Adversarial
Hints
Focusing attention
Quantization smoothing
Predicting inputs
Using the future to predict the present
Representation learning
What auxiliary tasks are helpful?
Conclusion