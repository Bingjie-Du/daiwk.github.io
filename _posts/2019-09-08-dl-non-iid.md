---
layout: post
category: "dl"
title: "非独立同分布的机器学习方法"
tags: [non-iid, 非独立同分布, 联邦学习, federated learning, ]
---

目录

<!-- TOC -->


<!-- /TOC -->

联邦学习相关可以看：[https://daiwk.github.io/posts/dl-federated-learning.html](https://daiwk.github.io/posts/dl-federated-learning.html)

[若DL没了独立同分布假设，样本不独立的机器学习方法综述](https://mp.weixin.qq.com/s/BCQYdpgZdzdNLXZadptP1w)

+ [Learning Classifiers When The Training Data Is Not IID](http://people.ee.duke.edu/~lcarin/IJCAI07-121.pdf)，ijcai，主要解决经典统计分析进行分类器预测过程中针对 Non-IID 数据的处理方法
+ [Communication-Efficient Learning of Deep Networks from Decentralized Data](http://arxiv.org/abs/1602.05629) 为解决联邦学习中 Non-IID 数据问题，提出一种基于迭代模型平均的深层网络联合学习方法（Federated Averaging，FedAvg）
+ [Federated Learning with Non-IID Data](https://arxiv.org/abs/1806.00582)是针对（2）的分析和改进，使用客户端数据分布和中央服务器数据总体分布之间的土方运距 (earth mover』s distance, EMD) 计算权重散度，同时提出了一种数据共享（Data-Sharing）策略改进 FedAvg 的性能
+ [On the Convergence of FedAvg on Non-IID Data](https://arxiv.org/abs/1907.02189)重点讨论联邦学习问题中 FedAvg 在处理 Non-IID 数据时的收敛性问题，从理论角度证明了 FedAvg 的有效性
+ [LoAdaBoost:Loss-Based AdaBoost Federated Machine Learning on medical data](http://arxiv.org/abs/1811.12629v2)基于 FedAvg 和数据共享策略提出了一种针对医学数据的提高联邦学习效率的自适应增强方法
