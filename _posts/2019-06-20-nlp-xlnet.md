---
layout: post
category: "nlp"
title: "XLNet"
tags: [xlnet, ]
---

目录

<!-- TOC -->


<!-- /TOC -->

[20项任务全面碾压BERT，CMU全新XLNet预训练模型屠榜（已开源）](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650764408&idx=1&sn=92196097be1a5f993ef02de0bac8128d&chksm=871ab006b06d3910ec88e57598d6c8b1a38dead073b3f417b793ba71ac4750ae2a8263537fa2&mpshare=1&scene=1&srcid=&pass_ticket=%2BD9Ask8qPVeDCkEHTF8NEBVBQX9YmDDkPy9VdMIfOYJ2VtpyHOOhIYdS3wUnvPjn#rd)

[XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/pdf/1906.08237.pdf)

预训练模型：[https://github.com/zihangdai/xlnet](https://github.com/zihangdai/xlnet)

BERT 这样基于去噪自编码器的预训练模型可以很好地建模双向语境信息，性能优于基于自回归语言模型的预训练方法。然而，由于需要 mask 一部分输入，BERT 忽略了被 mask 位置之间的依赖关系，因此出现预训练和微调效果的差异（pretrain-finetune discrepancy）。

基于这些优缺点，该研究提出了一种泛化的自回归预训练模型XLNet。XLNet可以：

+ 通过最大化所有可能的因式分解顺序的对数似然，学习双向语境信息；
+ 用自回归本身的特点克服BERT的缺点。此外，XLNet 还融合了当前最优自回归模型 Transformer-XL 的思路。

来，看一下transformer-xl：[https://daiwk.github.io/posts/nlp-transformer-xl.html](https://daiwk.github.io/posts/nlp-transformer-xl.html).。。。原来transformer-xl也是这几个作者提出的。。。

XLNet 在 20 个任务上超过了 BERT 的表现，并在 18 个任务上取得了当前最佳效果（state-of-the-art），包括机器问答、自然语言推断、情感分析和文档排序。

作者从自回归（autoregressive）和自编码（autoencoding）两大范式分析了当前的预训练语言模型，并发现它们虽然各自都有优势，但也都有难以解决的困难。为此，研究者提出 XLNet，并希望结合大阵营的优秀属性。
