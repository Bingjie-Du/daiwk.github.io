---
layout: post
category: "nlp"
title: "nmt"
tags: [nmt, ]
---

* [GRU](#gru)
* [encoder-decoder框架](#encoder-decoder框架)
     * [encoder](#encoder)
     * [decoder](#decoder)


paddle rnn系列文档：
[https://github.com/PaddlePaddle/Paddle/tree/develop/doc/howto/deep_model/rnn ](https://github.com/PaddlePaddle/Paddle/tree/develop/doc/howto/deep_model/rnn )

## GRU

GRU是Cho等人在LSTM上提出的简化版本，也是RNN的一种扩展，如下图所示。GRU单元只有两个门：

+ 重置门（reset gate）：如果重置门关闭，会忽略掉历史信息，即历史不相干的信息不会影响未来的输出。
+ 更新门（update gate）：将LSTM的输入门和遗忘门合并，用于控制历史信息对当前时刻隐层输出的影响。如果更新门接近1，会把历史信息传递下去。

<html>
<br/>

<img src='../assets/gru.png' style='max-height: 350px;max-width:500px'/>
<br/>

</html>

## 双向GRU

我们已经在语义角色标注一章中介绍了一种双向循环神经网络，这里介绍Bengio团队在论文[2,4]中提出的另一种结构。该结构的目的是输入一个序列，得到其在每个时刻的特征表示，即输出的每个时刻都用定长向量表示到该时刻的上下文语义信息。

具体来说，该双向循环神经网络分别在时间维以顺序和逆序——即前向（forward）和后向（backward）——依次处理输入序列，并将每个时间步RNN的输出拼接成为最终的输出层。这样每个时间步的输出节点，都包含了输入序列中当前时刻完整的过去和未来的上下文信息。下图展示的是一个按时间步展开的双向循环神经网络。该网络包含一个前向和一个后向RNN，其中有六个权重矩阵：输入到前向隐层和后向隐层的权重矩阵（W1,W3），隐层到隐层自己的权重矩阵（W2,W5），前向隐层和后向隐层到输出层的权重矩阵（W4,W6）。注意，该网络的前向隐层和后向隐层之间没有连接。

<html>
<br/>

<img src='../assets/bi-lstm-bengio.png' style='max-height: 350px;max-width:500px'/>
<br/>

</html>

## encoder-decoder框架

编码器-解码器（Encoder-Decoder）([Learning phrase representations using RNN encoder-decoder for statistical machine translation](http://www.aclweb.org/anthology/D/D14/D14-1179.pdf))框架用于解决由一个任意长度的源序列到另一个任意长度的目标序列的变换问题。即编码阶段将整个源序列编码成一个向量，解码阶段通过最大化预测序列概率，从中解码出整个目标序列。编码和解码的过程通常都使用RNN实现。

<html>
<br/>

<img src='../assets/encoder-decoder.png' style='max-height: 350px;max-width:500px'/>
<br/>

</html>

### encoder

总体流程如下：
<html>
<br/>

<img src='../assets/encoder-process.png' style='max-height: 350px;max-width:600px'/>
<br/>

</html>

重点在于：
最后对于词xi，通过拼接两个GRU的结果得到它的隐层状态。

<html>
<br/>

<img src='../assets/encoder.png' style='max-height: 350px;max-width:500px'/>
<br/>

</html>

### decoder

<html>
<br/>

<img src='../assets/decoder-process.png' style='max-height: 350px;max-width:600px'/>
<br/>

</html>

关键在于，预测的`\(z_{i+1}\)`由以下三部分经过**非线性激活**产生：
+ 源语言句子的**编码信息**`\(c\)`(`\(c=qh\)`，如果不用注意力，可以直接取最后一个时间步的编码`\(c=h_T\)`，也可以用时间维上的pooling的结果)。
+ **真实目标语言序列**的第i个词`\(u_i\)`。`\(u_0\)`是开始标志```<s>```。
+ i时刻的RNN**隐层状态**`\(z_i\)`, `\(z_0\)`是全零向量。

## 注意力机制

如果在编码阶段的输出是一个固定维度的向量，会有以下两个问题：

+ 不论源语言序列长度是5个词还是50个词，如果都用固定长度的向量去编码其中的语义和句法信息，对模型来讲，**是一个非常高的要求，特别对长句子而言。**
+ 直觉上，当人类翻译一句话时，会对**与【当前】译文更相关的源语言片段给予更多关注，且关注点会随翻译的进行而改变。**

而固定维度的向量相当于，任何时刻都对源语言所有信息给予同等程度的关注，Bahdanau等人[Neural Machine Translation by Jointly Learning to Align and Translate, ICLR, 2015](https://arxiv.org/pdf/1409.0473.pdf)引入注意力机制，对编码后的上下文片段进行解码，以此解决长句子的特征学习问题。

和简单的编码器不同之处在于，前面生成`\(z_{i+1}\)`用的是`\(c\)`，而这里用的是`\(c_i\)`，也就是，对每个真实目标语言序列中的词`\(u_i\)`【注意！！是目标语言的i，不是源语言的！！】，都有一个特定的`\(c_i\)`与之对应。

`\[
z_{i+1}=\phi _{\theta }(c_i, u_i, z_i)
\]`

`\[
c_i=\sum ^T_{j=1}a_{ij}h_j, a_i=[a_{i1}, a_{i2}, ..., a_{iT}]
\]`

可见，注意力机制是通过**对编码器中各时刻的RNN状态**`\(h_j\)`进行加权平均实现的。权重`\(a_{ij}\)`计算方法如下：

`\[
c_i=\sum ^T_{j=1}a_{ij}h_j, a_i=[a_{i1}, a_{i2}, ..., a_{iT}]
\]`

`\[
a_{ij}=\frac {exp(e_{ij})}{\sum _{k=1}^Texp(e_{ik})}
\]`

`\[\
e_{ij}=align(z_i,h_j)
\]`

其中，align可以看作一个对齐模型，用于衡量**目标语言第i个词**（第i个隐层状态`\(z_i\)`）和**源语言第j个词**（第j个词的上下文片段`\(h_j\)`）的匹配程度。

传统的对齐模型中，目标语言的每个词明确对应源语言的一个词或多个词（hard alignment）；而这里用的是soft alignment，即**任何两个目标语言和源语言词间**均存在一定的关联（模型计算出的实数值）。

<html>
<br/>

<img src='../assets/decoder-attention.png' style='max-height: 350px;max-width:500px'/>
<br/>

</html>

