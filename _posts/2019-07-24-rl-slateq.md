---
layout: post
category: "rl"
title: "SLATEQ"
tags: [slateq, youtube, ]
---

目录

<!-- TOC -->

- [背景](#%e8%83%8c%e6%99%af)
- [结合top-k off-policy一起看](#%e7%bb%93%e5%90%88top-k-off-policy%e4%b8%80%e8%b5%b7%e7%9c%8b)

<!-- /TOC -->

[Reinforcement Learning for Slate-based Recommender Systems: A Tractable Decomposition and Practical Methodology](https://arxiv.org/pdf/1905.12767.pdf)

这个比较长。。38页

[SLATEQ: A Tractable Decomposition for Reinforcement Learning with Recommendation Sets](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/9f91de1fa0ac351ecb12e4062a37afb896aa1463.pdf)

这个比较短。。

## 背景

xxx

## 结合top-k off-policy一起看

[以 YouTube 论文学习如何在推荐场景应用强化学习](https://zhuanlan.zhihu.com/p/77494496)

一个是 off-policy，一个是 value-base，用 on-policy。

好在推荐场景的样本收集成本低，量级比较大，但问题是存在较为**严重的Bias**。即只有被系统**展示过的物料才有反馈**，而且，还会有源源不断的**新物料和用户**加入。

off-policy的特点是，使用了两个policy，一个是用户behavior的`\(\beta\)`，代表产生用户行为Trajectory：`\((s_0,A_0,s_1, ..., )\)`的策略，另一个是系统决策的`\(\pi\)`，代表系统是如何在面对用户a在状态s下选择某个action的。

off-policy的好处是一定程度上带了exploration，但也带来了问题。因此，常见的是引入importance weighting来解决。

和标准的objective比，多了一个因子，因为这个**因子是连乘**和rnn的问题类似，**梯度容易爆炸或消失**。论文中用了一个近似解，并有人证明了是ok的。

value-base虽然直观容易理解，但一直被质疑不能稳定的收敛。

而policy-base则有较好的收敛性质，所以在很多推荐场景的RL应用，大部分会选择policy-base。当然现在也很有很多二者融合的策略，比如A3C、DDPG这种，也是比较流行的。

`\(\pi\)`的训练是比较常规的，有意思的是`\(\beta\)`的学习。用户的behavior是很难建模的，我们还是用nn的方式去学一个出来，这里有一个单独的分支去预估`\(\beta\)`，和`\(\pi\)`是一个网络，但是它的梯度不回传。

listwise的loss并不容易优化，复杂度较高。RL在推荐场景，也会遇到相同的问题。但直接做list推荐是不现实的，假设我们一次推荐K个物料，总共有N个物料，那么我们能选择的action就是一个排列组合问题，`\(C_N^K * K!\)`个，当N是百万级时，量级非常夸张。

youtube的两篇论文，都将问题从listwise（他们叫slatewise）转化成了itemwise。但这个itemwise和我们常规理解的pointwise的个性化技术还是有区别的。在于这个wise是reward上的表达，同时要引申出user choice model。

pointwise的方法只考虑单个item的概率，论文中提出的itemwise，虽然也是认为最后的reward只和每个被选中的item有关，且item直接不互相影响，但它有对user choice做假设。比如论文[2]还做了更详细的假设，将目标函数的优化变成一个多项式内可解的问题

SC是指用户一次指选择一个item，RTDS是指reward只和当前选择的item有关。

有不少研究是专门针对user choice model的，一般在经济学中比较多。推荐中常见的有cascade model和mutilnomial logit model，比如cascade model，会认为用户选择某个item的概率是p，那么在一个list下滑的过程中，点击了第j个item的概率是`\((1-p(i))^j * p(j)\)`.

论文1中最后的objective中有一个因子，表达了user choice的假设：

`\[
\lambda_{K}\left(s_{t}, a_{t}\right)=\frac{\partial \alpha\left(a_{t} | s_{t}\right)}{\partial \pi\left(a_{t} | s_{t}\right)}=K\left(1-\pi_{\theta}\left(a_{t} | s_{t}\right)\right)^{K-1}
\]`

简单理解就是，用`\(\pi\)`当做用户每次选择的概率，那上面就是K-1不选择a概率的连乘。而论文2中，RL模型和现有的监督模型是融合在一起的，直接用pCTR模型预估的pctr来当这个user choice的概率。
