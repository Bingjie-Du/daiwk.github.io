---
layout: post
category: "nlp"
title: "Modeling Interestingness with Deep Neural Networks(DSSM)"
tags: [Modeling Interestingness with Deep Neural Networks, DSSM]
---

目录

<!-- TOC -->

- [2013年CIKM的dssm](#2013%E5%B9%B4cikm%E7%9A%84dssm)
- [2014年CIKM的dssm: 引入cnn](#2014%E5%B9%B4cikm%E7%9A%84dssm-%E5%BC%95%E5%85%A5cnn)
- [2014年的dssm](#2014%E5%B9%B4%E7%9A%84dssm)
- [语义匹配](#%E8%AF%AD%E4%B9%89%E5%8C%B9%E9%85%8D)

<!-- /TOC -->

参考[Modeling Interestingness with Deep Neural Networks](https://www.microsoft.com/en-us/research/wp-content/uploads/2014/10/604_Paper.pdf)

对应的ppt：
[ppt](../assets/Modeling Interestingness with Deep Neural Networks_ppt.pdf)

## 2013年CIKM的dssm

[Learning Deep Structured Semantic Models for Web Search using Clickthrough Data](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/cikm2013_DSSM_fullversion.pdf)

相当于一个q，和每个d分别算cos。

<html>
<br/>

<img src='../assets/dssm_2013cikm.png' style='max-height: 350px'/>
<br/>

</html>

## 2014年CIKM的dssm: 引入cnn

[A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval](http://www.iro.umontreal.ca/~lisa/pointeurs/ir0895-he-2.pdf)

## 2014年的dssm

为用户的『兴趣』建模：
+ 自动高亮
+ 上下文实体检索

[Modeling Interestingness with Deep Neural Networks](https://www.microsoft.com/en-us/research/wp-content/uploads/2014/10/604_Paper.pdf)


paddle上，基于dssm，做了支持pairwise rank和分类/回归的一个通用模型：
[https://github.com/PaddlePaddle/models/blob/develop/dssm/README.cn.md](https://github.com/PaddlePaddle/models/blob/develop/dssm/README.cn.md)


## 语义匹配

[https://blog.csdn.net/zjrn1027/article/details/80170966](https://blog.csdn.net/zjrn1027/article/details/80170966)

相似度衡量可以使用cos，而最终的loss可以用hinge loss：

设置一个margin `\(m\)`，query `\(V_Q\)`，正样本`\(V_{A^+}\)`，负样本`\(V_{A^-}\)`，如果**正负样本的相似度之差**小于边界值，那就还需要优化，如果**已经大于等于边界值**了，说明**模型已经能区分**了，所以用hinge loss：

`\[
L=max(0, m - (cos(V_Q,V_{A^+} - cos(V_Q, V_{A^-}))))
\]`

tf算cos

```python
    def getCosineSimilarity(q, a):
        q1 = tf.sqrt(tf.reduce_sum(tf.multiply(q, q), 1))
        a1 = tf.sqrt(tf.reduce_sum(tf.multiply(a, a), 1))
        mul = tf.reduce_sum(tf.multiply(q, a), 1)
        cosSim = tf.div(mul, tf.multiply(q1, a1))
        return cosSim
```

tf算hinge

```python
    def getLoss(trueCosSim, falseCosSim, margin):
        zero = tf.fill(tf.shape(trueCosSim), 0.0)
        tfMargin = tf.fill(tf.shape(trueCosSim), margin)
        with tf.name_scope("loss"):
            losses = tf.maximum(zero, tf.subtract(tfMargin, tf.subtract(trueCosSim, falseCosSim)))
            loss = tf.reduce_sum(losses)
        return loss
```

使用

```python
        self.trueCosSim = self.getCosineSimilarity(question2, trueAnswer2)
        self.falseCosSim = self.getCosineSimilarity(question2, falseAnswer2)
        self.loss = self.getLoss(self.trueCosSim, self.falseCosSim, self.margin)
```
