---
layout: post
category: "dl"
title: "multi-task learning"
tags: [mtl, multi-task learning, 多目标 ]
---

目录

<!-- TOC -->

- [tags: [mtl, multi-task learning, 多目标 ]](#tags-mtl-multi-task-learning-%E5%A4%9A%E7%9B%AE%E6%A0%87)
- [两大类mtl方法](#%E4%B8%A4%E5%A4%A7%E7%B1%BBmtl%E6%96%B9%E6%B3%95)
  - [hard参数共享](#hard%E5%8F%82%E6%95%B0%E5%85%B1%E4%BA%AB)
  - [soft参数共享](#soft%E5%8F%82%E6%95%B0%E5%85%B1%E4%BA%AB)
- [mtl为什么会work](#mtl%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9Awork)
  - [隐式数据增强](#%E9%9A%90%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA)
  - [Attention focusing](#attention-focusing)
  - [Eavesdropping](#eavesdropping)
  - [Representation bias](#representation-bias)
  - [Regularization](#regularization)
- [非神经网络中的mtl](#%E9%9D%9E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84mtl)
  - [Block-sparse regularization](#block-sparse-regularization)
  - [Learning task relationships](#learning-task-relationships)
- [MTL Deep Learning的Recent work](#mtl-deep-learning%E7%9A%84recent-work)
  - [Deep Relationship Networks](#deep-relationship-networks)
  - [Fully-Adaptive Feature Sharing](#fully-adaptive-feature-sharing)
  - [Cross-stitch Networks](#cross-stitch-networks)
  - [Low supervision](#low-supervision)
  - [A Joint Many-Task model](#a-joint-many-task-model)
  - [Weighting losses with uncertainty](#weighting-losses-with-uncertainty)
  - [Tensor factorisation for MTL](#tensor-factorisation-for-mtl)
  - [Sluice Networks](#sluice-networks)
  - [What should I share in my model?](#what-should-i-share-in-my-model)
- [Auxiliary tasks](#auxiliary-tasks)

<!-- /TOC -->

参考：

+ [http://ruder.io/multi-task/](http://ruder.io/multi-task/)


## 两大类mtl方法

### hard参数共享

所有task共享大的隐层表示，然后每个task有自己的output层。可以极大地减少过拟合，因为大的隐层表示的参数量比每个task自己的小output的参数量要大得多。

<html>
<br/>
<img src='../assets/mtl-hard.png' style='max-height: 200px'/>
<br/>
</html>

### soft参数共享

每个task有自己的模型和自己的参数。模型参数间的距离一般会使用**`\(\ell_2\)`正则**进行正则化，目的是为了让参数间更相似。

<html>
<br/>
<img src='../assets/mtl-soft.png' style='max-height: 200px'/>
<br/>
</html>

## mtl为什么会work

假设task A和task B，有共同的隐层表示F。

### 隐式数据增强

### Attention focusing

### Eavesdropping

### Representation bias

### Regularization

## 非神经网络中的mtl

### Block-sparse regularization

### Learning task relationships

## MTL Deep Learning的Recent work 

### Deep Relationship Networks

### Fully-Adaptive Feature Sharing

### Cross-stitch Networks

### Low supervision

### A Joint Many-Task model

### Weighting losses with uncertainty

### Tensor factorisation for MTL

### Sluice Networks

### What should I share in my model?

## Auxiliary tasks
Related task
Adversarial
Hints
Focusing attention
Quantization smoothing
Predicting inputs
Using the future to predict the present
Representation learning
What auxiliary tasks are helpful?
Conclusion