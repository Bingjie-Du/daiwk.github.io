---
layout: post
category: "dl"
title: "MixHop"
tags: [mixhop, ]
---

目录

<!-- TOC -->


<!-- /TOC -->

参考[节后收心困难？这15篇论文，让你迅速找回学习状态](https://mp.weixin.qq.com/s/aaz-s87vorroyepNCd9-AA)

[MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing](https://arxiv.org/abs/1905.00067)

[https://github.com/samihaija/mixhop](https://github.com/samihaija/mixhop)

本文是图分析大牛 Bryan Perozzi 组发表于 ICML 2019 的工作。本文 argue 现有的 GNN 模型无法学习到一种很通用的邻居混合信息，然后提出了 MixHop 来混合不同阶邻居的信息并学习节点表示。

MixHop 非常的高效并且有很强的理论背景（MixHop 与 delta operators 之间的联系）。另外，通过混合各阶信息，MixHop 一定程度上避免了 GNN 过平滑问题。GNN 的过平滑问题：随着层数的增加，GNN 所学习到的节点表示变的没有区分度。最后作者通过大量的试验验证了 MixHop 的效果。

在 Citeseer，Cora 和 Pubmed 上，MixHop 都取得了大量提升。例如，虽然 MixHop 没有使用注意力机制来学习邻居的重要性，但其表现依然大幅超过 GAT。

